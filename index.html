<!DOCTYPE html>
<html lang="en-GB">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Visual Computing Reading Group">

  <title>VCRG</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css"
    integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
  <link rel="stylesheet" href="style.css">
  <link rel="shortcut icon" type="image/png" href="https://img.icons8.com/flat_round/64/000000/bookmark-book.png" />
</head>

<body>

<div class="container">
  <h4 class="heading center_text">Bath Visual Computing<br>Reading Group and Research Seminar</h4>
  <hr>

  <p class="location_time center_text">
    <b>Wednesdays from 16:15 to 17:05 online on Teams</b>
  </p>

  <p>
    This page contains information and the schedule for the Bath Visual Computing reading group and research seminar.
    Most weeks, two students will give two different types of presentation:
  </p>

  <h5>Reading Group</h5>
  <p>
    A short presentation (~10 minutes) and group discussion (~10 minutes) of a usually recent research paper that is relevant to a student’s research and of more general interest to the wider group.
    This can also be a quick survey of a field, e.g. based on your confirmation report.
    You can prepare slides, if you like, or present using an existing video and the paper.
  </p>
  <p>
    <u>Preparation:</u>
    Please send the title of your chosen paper and a link to the project or paper to the organiser the week before your presentation.
    The idea is to allow others to look at the paper prior to the reading group on Wednesday.
  </p>

  <h5>Research Seminar</h5>
  <p>
    This is a short presentation (~10 minutes) on a student’s current work in progress, including the motivation and goal of the project, as well as a discussion of current issues and next steps, followed by questions and answers from the group (~10 minutes).
  </p>

  <hr>
  <h5>Ground rules</h5>
  <ol>
    <li>Timeslots were allocated randomly with manual tweaks to ensure at least 4 weeks between presentations by the same person.</li>
    <li>If a time is inconvenient for whatever reason, please find someone to swap with, who agrees to swap. Notify the organiser about the swap via email, ccing the other presenter.</li>
  </ol>

  <h4 class="center_text">Schedule for 2021</h4>
  <hr>

  <div class="table-wrapper">
    <table id="schedule" class="table table-hover table-striped">
      <thead>
        <tr>
          <th scope="col" width="10%">Date</th>
          <th scope="col" width="45%">Reading&nbsp;Group</th>
          <th scope="col" width="45%">Research&nbsp;Seminar</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>30&nbsp;Jun&nbsp;2021</td>
          <td colspan="2">We will start with a discussion about <a
              href="https://www.youtube.com/watch?v=w6Pw4MOzMuo">Michael Bronstein’s ICLR 2021 Keynote on
              Geometric Deep Learning</a>. Please watch the video (~40 minutes) before the reading group.</td>
        </tr>
        <tr>
          <td>7&nbsp;Jul&nbsp;2021</td>
          <td colspan="2"><a href="https://filebox.ece.vt.edu/~jbhuang/">Jia-Bin Huang</a> summarises <a
              href="https://youtu.be/kzmR3Njtwdg">recent work on dynamic NeRFs</a>, which will be the basis of
            our discussion this week. Please watch the video (~50 minutes + discussion) beforehand.</td>
        </tr>
        <tr>
          <td>14&nbsp;Jul&nbsp;2021</td>
          <td>Jack R. Saunders:<br><a href="https://gvv.mpi-inf.mpg.de/projects/NeuralStylePreservingVisualDubbing/"><b>Neural Style-Preserving Visual Dubbing</b></a><br>[Kim et al., SIGGRAPH Asia 2019]</td>
          <td>Alex Rotsidis:<br><b>Improving feature matching times by discarding non-matchable features</b></td>
        </tr>
        <tr>
          <td>21&nbsp;Jul&nbsp;2021</td>
          <td>Sameh Hussain:<br><a href="https://compvis.github.io/brushstroke-parameterized-style-transfer/"><b>Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes</b></a> [Kotovenko and Wright et al., CVPR 2021]</td>
          <td>Tina Zhou:<br><b>Deep learning models in comparing the simulated solar magnetic field and satellite observations</b></td>
        </tr>
        <tr>
          <td>28&nbsp;Jul&nbsp;2021</td>
          <td>Ningchao Wang:<br><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Synthesizing_3D_Shapes_From_Silhouette_Image_Collections_Using_Multi-Projection_Generative_CVPR_2019_paper.pdf"><b>Synthesizing 3D Shapes from Silhouette Image Collections using Multi-projection Generative Adversarial Networks</b></a> [Li et al., CVPR 2019]</td>
          <td>Xingyun Yang:<br><b>Cross domain object detection</b></td>
        </tr>
        <tr>
          <td>4&nbsp;Aug&nbsp;2021</td>
          <td>Jundan Luo:<br><a href="https://factorize-a-city.github.io/"><b>Learning to Factorize and Relight a City</b></a> [Liu et al., ECCV 2020]</td>
          <td>Xi Tian:<br><b>Person-in-Context Synthesis</b></td>
        </tr>
        <tr>
          <td>11&nbsp;Aug&nbsp;2021</td>
          <td>Jake Deane:<br><a href="https://arxiv.org/abs/2104.10916"><b>Automated Tackle Injury Risk Assessment in Contact-Based Sports – A Rugby Union Example</b></a> [Martin et al., CVPR Computer Vision in Sports Workshop 2021]</td>
          <td>Oscar Bryan:<br><b>Machine Learning for the detection of unexploded ordnance using Synthetic Aperture Sonar</b></td>
        </tr>
        <tr>
          <td>18&nbsp;Aug&nbsp;2021</td>
          <td>Ed Wong:<br><a href="https://openreview.net/forum?id=dFwBosAcJkN"><b>Perceptual Adversarial Robustness: Defense Against Unseen Threat Models</b></a> [Laidlaw et al., ICLR 2021]</td>
          <td>Ketan Fatania:<br><b>Plug and Play ADMM with CNN Denoiser Priors for Quantitative MRI Image Reconstruction</b></td>
        </tr>
        <tr>
          <td>25&nbsp;Aug&nbsp;2021</td>
          <td>Tina Zhou:<br><a href="https://arxiv.org/abs/1307.5551"><b>Regularized Discrete Optimal Transport</b></a> [Ferradans et al., SIAM Journal on Imaging Sciences 2014]</td>
          <td>—</td>
        </tr>
        <tr>
          <td>1&nbsp;Sep&nbsp;2021</td>
          <td>Xiaochang Liu:<br><a href="https://www.cs.cornell.edu/projects/style-painting-robustness/"><b>What Can Style Transfer and Paintings Do For Model Robustness?</b></a> [Lin et al., CVPR 2021]</td>
          <td>Qi Zhang:<br><b>SLAM structure</b></td>
        </tr>
        <tr>
          <td>8&nbsp;Sep&nbsp;2021</td>
          <td>Oscar Bryan:<br><a href="https://github.com/oscarkey/explanations-by-minimizing-uncertainty"><b>Generating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties</b></a> [Schut, Key et al., AISTATS 2021]</td>
          <td>Jundan Luo:<br><b>Scene-level intrinsic image decomposition</b></td>
        </tr>
        <tr>
          <td>15&nbsp;Sep&nbsp;2021</td>
          <td>—</td>
          <td>Joe Goodier:<br><b>Interpretable Diagnostic Imaging using Generative Models</b></td>
        </tr>
        <tr>
          <td>22&nbsp;Sep&nbsp;2021</td>
          <td>Xingyun Yang:<br><a href="https://arxiv.org/abs/2005.12872"><b>End-to-End Object Detection with Transformers</b></a> [Carion and Massa et al., ECCV 2020]</td>
          <td>Padraig Boulton:<br><b>Recognition of Specific Objects Regardless of Depiction</b></td>
        </tr>
        <tr>
          <td>29&nbsp;Sep&nbsp;2021</td>
          <td>Qi Zhang:<br><a href="https://devendrachaplot.github.io/projects/Neural-SLAM"><b>Learning to Explore using Active Neural SLAM</b></a> [Chaplot et al., ICLR 2020]</td>
          <td>Ningchao Wang:<br><b>Learning styles of interior design with attention</b></td>
        </tr>
        <tr>
          <td>6&nbsp;Oct&nbsp;2021</td>
          <td>George Fletcher:<br><a href="https://github.com/sebastianstarke/AI4Animation#siggraph-2018mode-adaptive-neural-networks-for-quadruped-motion-controlhe-zhangsebastian-starketaku-komurajun-saitoacm-trans-graph-37-4-article-145joint-first-authors"><b>Mode-Adaptive Neural Networks for Quadruped Motion Control</b></a> [Zhang and Starke et al., SIGGRAPH 2018]</td>
          <td>Jack R. Saunders:<br><b>Style in Facial Animation</b></td>
        </tr>
        <tr>
          <td>13&nbsp;Oct&nbsp;2021</td>
          <td>Will Kerr:<br><a href="https://arxiv.org/abs/1909.01040"><b>A Geometry-Sensitive Approach for Photographic Style Classification</b></a> [Ghosal et al., IMVIP 2018]</td>
          <td>George Fletcher:<br><b>Motion Style Transfer</b></td>
        </tr>
        <tr>
          <td>20&nbsp;Oct&nbsp;2021</td>
          <td>Joe Goodier:<br><a href="https://arxiv.org/abs/2105.05233"><b>Diffusion Models Beat GANs on Image Synthesis</b></a> [Dhariwal and Nichol, arXiv 2021]</td>
          <td>Avery Huang:<br><b>Computational method for modelling assembly developable surfaces</b></td>
        </tr>
        <tr>
          <td>27&nbsp;Oct&nbsp;2021</td>
          <td>Xi Tian:<br><a href="https://compvis.github.io/taming-transformers/"><b>Taming Transformers for High-Resolution Image Synthesis</b></a> [Esser et al., CVPR 2021]</td>
          <td>Jake Deane</td>
        </tr>
        <tr>
          <td>3&nbsp;Nov&nbsp;2021</td>
          <td>Dan Merchant</td>
          <td>Alex Taylor</td>
        </tr>
        <tr>
          <td>10&nbsp;Nov&nbsp;2021</td>
          <td>Padraig Boulton</td>
          <td>Manuel Rey Area</td>
        </tr>
        <tr>
          <td>17&nbsp;Nov&nbsp;2021</td>
          <td>Fei (Avery) Huang</td>
          <td>Xiaochang Liu</td>
        </tr>
        <tr>
          <td>24&nbsp;Nov&nbsp;2021</td>
          <td>Alex Taylor</td>
          <td>Will Kerr</td>
        </tr>
        <tr>
          <td>1&nbsp;Dec&nbsp;2021</td>
          <td>Alex Rotsidis</td>
          <td>Ed Wong</td>
        </tr>
        <tr>
          <td>8&nbsp;Dec&nbsp;2021</td>
          <td>Manuel Rey Area</td>
          <td>Dan Merchant</td>
        </tr>
        <!-- <tr>
          <td>15&nbsp;Dec&nbsp;2021</td>
          <td>Name</td>
          <td>Name</td>
        </tr> -->
      </tbody>
    </table>
  </div>
</div>

</body>
</html>
